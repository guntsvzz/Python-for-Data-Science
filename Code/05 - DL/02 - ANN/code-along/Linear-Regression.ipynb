{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch for Linear Regression\n",
    "\n",
    "So we can understand how PyTorch works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#as you all know, things can speed up if you have NVIDIA GPU\n",
    "#CUDA is the framework that NVIDIA develops, which allows us to use the GPU for calculations\n",
    "\n",
    "#my PC is MAC, and I don't have CUDA\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan for today:\n",
    "\n",
    "1. ETL \n",
    "   1. Specifying some some random input\n",
    "   2. PyTorch Dataset and DataLoader\n",
    "2. EDA - we gonna just skip because we are lazy...\n",
    "3. Feature Engineering / Cleaning - which we don't need to....\n",
    "4. Modeling \n",
    "   1. `nn.Linear` (luckily, you already understand this!  Yay!)\n",
    "   2. Define loss function (mse for regression, ce for classification)\n",
    "   3. Define the optimizer function (gradient descent ; adam)\n",
    "   4. Train the model\n",
    "5. Inference / Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ETL\n",
    "\n",
    "### 1.1 Specify some input\n",
    "\n",
    "Consider this data:\n",
    "\n",
    "<img src = \"../figures/japan.png\" width=\"500\">\n",
    "\n",
    "In a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :\n",
    "\n",
    "$$\\text{yield}_\\text{apple}  = w_{11} * \\text{temp} + w_{12} * \\text{rainfall} + w_{13} * \\text{humidity} + b_{1}$$\n",
    "\n",
    "$$\\text{yield}_\\text{orange} = w_{21} * \\text{temp} + w_{22} * \\text{rainfall} + w_{23} * \\text{humidity} + b_{2}$$\n",
    "\n",
    "Visually, it means that the yield of apples is a linear or planar function of temperature, rainfall and humidity:\n",
    "\n",
    "<img src = \"../figures/japan2.png\" width=\"400\">\n",
    "\n",
    "The learning part of linear regression is to figure out a set of weights <code>w11, w12,... w23, b1 \\& b2</code> using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X (temp, rainfall, hum)\n",
    "X_train = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
    "                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n",
    "                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
    "                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n",
    "                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "Y_train = np.array([[56, 70], [81, 101], [119, 133], \n",
    "                    [22, 37], [103, 119], [56, 70], \n",
    "                    [81, 101], [119, 133], [22, 37], \n",
    "                    [103, 119], [56, 70], [81, 101], \n",
    "                    [119, 133], [22, 37], [103, 119]], \n",
    "                   dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15, 3]), torch.Size([15, 2]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#please create tensors from these numpy array\n",
    "#torch.from_numpy (copy)  or torch.tensor  (not a copy!)\n",
    "inputs  = torch.tensor(X_train)\n",
    "targets = torch.tensor(Y_train)\n",
    "\n",
    "#please print the shape of these tensors\n",
    "#use either .size() or .shape\n",
    "inputs.shape, targets.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset\n",
    "\n",
    "We gonna create `TensorDataset` on top of these tensors, so we can access each row from inputs and targets as tuples.   \n",
    "\n",
    "Note:  This must be done, if we want to use `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put this dataset on top of our inputs and targets\n",
    "#format: TensorDataset(X, y) where X.shape is (m, n) and y.shape is (m, k)\n",
    "ds = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([91., 88., 64.]), tensor([ 81., 101.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[1] #this is a tuple of two tensors, the x and the corresponding y\n",
    "#this IS THE FORMAT that pyTorch wants!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 DataLoader\n",
    "\n",
    "By default, PyTorch works in batch (remember the mini-batch gradient descent!).\n",
    "\n",
    "In simple words, it will ALWAYS take some mini-batch, and perform gradient descent.\n",
    "\n",
    "Why PyTorch assume mini-batch; because PyTorch assumes you won't be able to fit in ~1M samples into your GPU ram....(3, 4, 6, 11, 12, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this dataloader will automatically create an enumerator, look at each batch\n",
    "#means, you can simply perform a for loop onto this dataloader\n",
    "#if you DON'T WANT TO use this DataLoader, it's fine!  But you have\n",
    "#to manually select the mini-batch (just like we do in our LR mini-batch class)\n",
    "from torch.utils.data import DataLoader  #this guy is randomized (if you set Shuffle=true)\n",
    "\n",
    "batch_size = 3  #this is any number you like\n",
    "#too small then your code runs slow\n",
    "#too big then you may get \"out of memory\" error\n",
    "\n",
    "dl = DataLoader(ds, batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, this dl is basically an enumerator, in which we can loop on....\n",
    "\n",
    "# for x, y in dl:\n",
    "#     print(f\"X: {x}\")  \n",
    "#     print(f\"Y: {y}\")\n",
    "#     break\n",
    "\n",
    "#this dl has an internal counter, that keeps where it is currently\n",
    "#this dl keeps on running; which is intentional; because we have the concept of \"epochs\"\n",
    "#epochs means that how many times we \"exhaust\" the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA - skip because we are lazy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define our neural network\n",
    "\n",
    "- how many linear layers we want???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn #stands for neural network; modules that contains many possible layers\n",
    "#define our neural network\n",
    "#just use one layer....\n",
    "#we gonna come back here and add one more layer....\n",
    "#format: nn.Linear(in_features, out_features)\n",
    "#format: nn.Linear(temp;rainfall;hum  ,  orange; apples)\n",
    "\n",
    "# model = nn.Linear(3, 2)\n",
    "\n",
    "#linear layers are basically simple matrix multiplication....\n",
    "#Many other names:  In Keras, we called Dense.  In TensorFlow, we called FullyConnected\n",
    "\n",
    "#Keras are very high-level - not good for research / development (mainly for education...)\n",
    "#TensorFlow is developed by Google - it's quite good\n",
    "\n",
    "#for very huge, complex, high performance model - TensorFlow is much better / optimized\n",
    "    #they are more low-level than PyTorch\n",
    "#for very generally almost any model that we use (even in research) - PyTorch is much better \n",
    "# due to its computational graph.....\n",
    "\n",
    "#TensorFlow supports something called TensorFlowLite, which is the way\n",
    "#you want to use for mobile phones...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wonder whether have one extra layer, can reduce the loss!\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(3, 10),\n",
    "#     nn.Linear(10, 2)\n",
    "# )  #this is fine, but this is not the best practice!!\n",
    "   #because in the future, there are many layers and complex stuffs in neural network...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class is the perfect and the best practice for creating a neural network of any type...\n",
    "\n",
    "#format:\n",
    "'''\n",
    "class AnyNameCapitalized(nn.Module): #it must inherit nn.Module\n",
    "    def __init__():\n",
    "        super().__init__()  #super is basically inheriting nn.Module init\n",
    "        #we define all the layers here.....\n",
    "        \n",
    "    def forward(self, x):   #YOU CANNOT CHANGE THIS NAME, it MUST BE \"forward\"\n",
    "        x = layer1()\n",
    "        x = layer2()\n",
    "        return x\n",
    "'''\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(  3  ,  10   ,  2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.weight\n",
    "#model.fc1.weight\n",
    "#model.fc2.weight\n",
    "# model.fc1.weight #by default, these weights are uniformly close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.weight.shape  #this one is basically in the shape (out_features, in_feature)\n",
    "\n",
    "#you can imagine X @ W^T\n",
    "#after you transpose W, W^T becomes [3, 2]\n",
    "#which now you can do X @ W^T which is (anything, 3) @ (3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.bias  #why two bias???\n",
    "# model.fc1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.parameters())  #this will list all the parameters (it's a object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p.numel() just flatten everything....\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "#why 8 here??? - 6 weights and 2 bias....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (fc1): Linear(in_features=3, out_features=10, bias=True)\n",
       "  (fc2): Linear(in_features=10, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so how do we use our model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so you can perform a forward pass, simply using \n",
    "#format: model(inputs)\n",
    "\n",
    "# print(\"Inputs: \", inputs.shape)\n",
    "\n",
    "# output = model(inputs)  #(15, 3) @ (3, 2) = (15, 2)\n",
    "# print(output.shape)  #why output.shape is 15, 2??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define the loss function\n",
    "\n",
    "- should we use MSE or Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#under the nn module, there are many loss function\n",
    "J_fn = nn.MSELoss()\n",
    "\n",
    "#later on, you will know how to use this....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Define the optimizer\n",
    "- Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normally, in sklearn, we simply call fit, and it will do gradient descent\n",
    "#in code from scratch, we need to like specify how we want to update the gradients\n",
    "#optimizer handles how we update the parameters\n",
    "#   if we use w = w - alpha (gradient) ==> gradient descent\n",
    "#optimizer is handles by the `torch.optim` module\n",
    "#stochastic gradient descent ==> is NOT one sample; is basically mini-batch \n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Actually train the model\n",
    "\n",
    "- 1. Predict\n",
    "- 2. Loss\n",
    "- 3. Gradient\n",
    "- 4. Update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 5  #it depends....trial and error....\n",
    "# #for num_epochs\n",
    "# for epoch in range(num_epochs):\n",
    "#     #for dataloader\n",
    "#     for x, y in dl:  \n",
    "#         #x: (batch, features) = (3, 3) \n",
    "#         #y: (batch, target)   = (3, 2)\n",
    "        \n",
    "#         #optional: you can put your x and y inside the CUDA (GPU) for speed up\n",
    "#         x.to(device)  #device is either cpu or cuda\n",
    "#         y.to(device)\n",
    "\n",
    "#         #1. predict (forward pass)\n",
    "#         yhat = model(x)\n",
    "        \n",
    "#         #2. calculate loss\n",
    "#         #sklearn.metrics.mse(y, yhat)\n",
    "#         #format: J_fn(inputs, targets)\n",
    "#         loss = J_fn(yhat, y)\n",
    "        \n",
    "#         #3. calculate gradient\n",
    "#         #3.1 clear out the previous gradients\n",
    "#         #format: optimizer.zero_grad()\n",
    "#         optim.zero_grad()\n",
    "        \n",
    "#         #3.2 called backward() on loss to retrieve all the gradients (backpropagation/backward pass)\n",
    "#         loss.backward()  #why called backward on loss!?\n",
    "#         #backward DOES NOT adjust the weight YET....just backpropagation\n",
    "#         #we want to calculate the gradients of all parameters (8 - 6 weights and 2 bias)\n",
    "#         #IN RESPECT TO THE LOSS....dJ/dw11,  dJ/dw12, dJ/dw13....., dJ/db1, dJ/db2\n",
    "        \n",
    "#         #4. update the parameters using the optim\n",
    "#         #W = W - alpha * gradient  #we don't need to do this here.....\n",
    "#         optim.step()  #optim already has learning rate, it also know all the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Loss: 9216.724609375\n",
      "Epoch: 0 - Loss: 2286.434326171875\n",
      "Epoch: 0 - Loss: 13370.09765625\n",
      "Epoch: 0 - Loss: 956.691162109375\n",
      "Epoch: 0 - Loss: 439.563232421875\n",
      "Epoch: 1 - Loss: 2167.588134765625\n",
      "Epoch: 1 - Loss: 3558.817626953125\n",
      "Epoch: 1 - Loss: 4449.04150390625\n",
      "Epoch: 1 - Loss: 13759.2548828125\n",
      "Epoch: 1 - Loss: 3169.0615234375\n",
      "Epoch: 2 - Loss: 7048.96484375\n",
      "Epoch: 2 - Loss: 11963.6806640625\n",
      "Epoch: 2 - Loss: 4749.7724609375\n",
      "Epoch: 2 - Loss: 4445.17529296875\n",
      "Epoch: 2 - Loss: 12233.6376953125\n",
      "Epoch: 3 - Loss: 6957.43505859375\n",
      "Epoch: 3 - Loss: 7230.873046875\n",
      "Epoch: 3 - Loss: 5775.482421875\n",
      "Epoch: 3 - Loss: 10892.2392578125\n",
      "Epoch: 3 - Loss: 10770.3076171875\n",
      "Epoch: 4 - Loss: 8404.9306640625\n",
      "Epoch: 4 - Loss: 9432.8291015625\n",
      "Epoch: 4 - Loss: 6948.67822265625\n",
      "Epoch: 4 - Loss: 7221.58642578125\n",
      "Epoch: 4 - Loss: 9582.5966796875\n",
      "Epoch: 5 - Loss: 8397.9658203125\n",
      "Epoch: 5 - Loss: 14723.15625\n",
      "Epoch: 5 - Loss: 7215.75048828125\n",
      "Epoch: 5 - Loss: 5457.076171875\n",
      "Epoch: 5 - Loss: 5761.20263671875\n",
      "Epoch: 6 - Loss: 9723.8212890625\n",
      "Epoch: 6 - Loss: 6905.66064453125\n",
      "Epoch: 6 - Loss: 1946.392578125\n",
      "Epoch: 6 - Loss: 10747.1474609375\n",
      "Epoch: 6 - Loss: 12196.6455078125\n",
      "Epoch: 7 - Loss: 9683.765625\n",
      "Epoch: 7 - Loss: 11919.1162109375\n",
      "Epoch: 7 - Loss: 4725.58544921875\n",
      "Epoch: 7 - Loss: 5750.99365234375\n",
      "Epoch: 7 - Loss: 9404.5576171875\n",
      "Epoch: 8 - Loss: 11007.2158203125\n",
      "Epoch: 8 - Loss: 9400.8564453125\n",
      "Epoch: 8 - Loss: 4720.84326171875\n",
      "Epoch: 8 - Loss: 11906.97265625\n",
      "Epoch: 8 - Loss: 4412.66796875\n",
      "Epoch: 9 - Loss: 7192.783203125\n",
      "Epoch: 9 - Loss: 7943.91845703125\n",
      "Epoch: 9 - Loss: 7190.28515625\n",
      "Epoch: 9 - Loss: 13347.7392578125\n",
      "Epoch: 9 - Loss: 5738.26513671875\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10 \n",
    "for epoch in range(num_epochs):\n",
    "    for x, y in dl:  \n",
    "        x.to(device)  #device is either cpu or cuda\n",
    "        y.to(device)\n",
    "\n",
    "        yhat = model(x)\n",
    "        loss = J_fn(yhat, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()  \n",
    "        optim.step()  \n",
    "        \n",
    "        print(f\"Epoch: {epoch} - Loss: {loss}\")\n",
    "        \n",
    "        #can you guys help tell what is the best hidden size?\n",
    "        #final exam is on Nov 22 9 - 12\n",
    "            #signal processing\n",
    "            #deep learning - pytorch\n",
    "        #project will be decided by Mr. Akraradet\n",
    "        #8 classes......14 lectures....\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference / Testing\n",
    "\n",
    "Test some random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[73., 67., 43.],\n",
       "         [91., 88., 64.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6155.0352, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#please create two numpy array of \n",
    "# [74, 68, 42], [92, 88, 65]\n",
    "x = np.array([[74, 68, 42], [92, 88, 65]], dtype='float32')\n",
    "#float  means 32 bits\n",
    "#double means 64 bits\n",
    "\n",
    "#please make it a tensor\n",
    "x_tensor = torch.tensor(x)\n",
    "\n",
    "#then use our model to predict the number of oranges and apples\n",
    "yhat = model(x_tensor)\n",
    "yhat\n",
    "\n",
    "#print the loss comparing with ds[0] and ds[1] - look at the y part ok...\n",
    "ytest = ds[0:2][1]\n",
    "loss = J_fn(yhat, ytest)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
